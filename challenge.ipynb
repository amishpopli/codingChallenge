{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment and load in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a python environment with pyspark , please use that and do not run the next 3 steps. Please directly move on to creating a spark context\n",
    "\n",
    "\n",
    "After running the setup-env script the kernel \"spark-env\" should show up in jupyter list of \n",
    "kernels. Please change the kernel before moving forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amishpopli/Desktop/projects/codingChallenge\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxr-xr-x   8 amishpopli  staff   256 Nov 16 12:31 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 amishpopli  staff   384 Nov 16 12:28 \u001b[34m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   9 amishpopli  staff   288 Nov 16 12:31 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 amishpopli  staff    39 Nov 16 12:31 .gitignore\r\n",
      "drwxr-xr-x   2 amishpopli  staff    64 Nov 16 12:31 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 amishpopli  staff  4994 Nov 16 12:20 challenge.ipynb\r\n",
      "-rw-r--r--   1 amishpopli  staff   385 Nov 16 10:27 requirements.txt\r\n",
      "-rw-r--r--   1 amishpopli  staff   503 Nov 16 10:27 setup-env.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a fresh environemnt\n",
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n",
      "Successfully installed pip-20.2.4\n",
      "Collecting appnope==0.1.0\n",
      "  Using cached appnope-0.1.0-py2.py3-none-any.whl (4.0 kB)\n",
      "Collecting backcall==0.2.0\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting decorator==4.4.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting ipykernel==5.3.4\n",
      "  Using cached ipykernel-5.3.4-py3-none-any.whl (120 kB)\n",
      "Collecting ipython==7.19.0\n",
      "  Using cached ipython-7.19.0-py3-none-any.whl (784 kB)\n",
      "Collecting ipython-genutils==0.2.0\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting jedi==0.17.2\n",
      "  Using cached jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting jupyter-client==6.1.7\n",
      "  Using cached jupyter_client-6.1.7-py3-none-any.whl (108 kB)\n",
      "Collecting jupyter-core==4.6.3\n",
      "  Using cached jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
      "Collecting parso==0.7.1\n",
      "  Using cached parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Collecting pexpect==4.8.0\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pickleshare==0.7.5\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting prompt-toolkit==3.0.8\n",
      "  Using cached prompt_toolkit-3.0.8-py3-none-any.whl (355 kB)\n",
      "Collecting ptyprocess==0.6.0\n",
      "  Using cached ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Collecting Pygments==2.7.2\n",
      "  Using cached Pygments-2.7.2-py3-none-any.whl (948 kB)\n",
      "Collecting pyspark==3.0.1\n",
      "  Using cached pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "Collecting python-dateutil==2.8.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pyzmq==20.0.0\n",
      "  Using cached pyzmq-20.0.0-cp38-cp38-macosx_10_9_x86_64.whl (814 kB)\n",
      "Collecting six==1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tornado==6.1\n",
      "  Using cached tornado-6.1-cp38-cp38-macosx_10_9_x86_64.whl (416 kB)\n",
      "Collecting traitlets==5.0.5\n",
      "  Using cached traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
      "Collecting wcwidth==0.2.5\n",
      "  Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./spark-env/lib/python3.8/site-packages (from ipython==7.19.0->-r requirements.txt (line 5)) (41.2.0)\n",
      "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: appnope, backcall, decorator, Pygments, ipython-genutils, traitlets, parso, jedi, ptyprocess, pexpect, wcwidth, prompt-toolkit, pickleshare, ipython, pyzmq, six, python-dateutil, jupyter-core, tornado, jupyter-client, ipykernel, py4j, pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed Pygments-2.7.2 appnope-0.1.0 backcall-0.2.0 decorator-4.4.2 ipykernel-5.3.4 ipython-7.19.0 ipython-genutils-0.2.0 jedi-0.17.2 jupyter-client-6.1.7 jupyter-core-4.6.3 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.8 ptyprocess-0.6.0 py4j-0.10.9 pyspark-3.0.1 python-dateutil-2.8.1 pyzmq-20.0.0 six-1.15.0 tornado-6.1 traitlets-5.0.5 wcwidth-0.2.5\n",
      "adding environmnet to jupyter notebook\n",
      "Installed kernelspec spark-env in /Users/amishpopli/Library/Jupyter/kernels/spark-env\n",
      "spark-env kernel created. Please change the kernel to use the latest spark-env kernel\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sh setup-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
